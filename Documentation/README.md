# Human-Cat Interaction Detector with iOS App

An AI-powered system to detect humans and cats using computer vision, with iOS app for remote monitoring. Designed for deployment on **Mac** and **NVIDIA Jetson Nano**.

## ğŸ¯ Project Overview

This system uses deep learning (YOLOv8) to:
- Detect **humans** and **cats** in real-time via webcam
- Auto-record interactions when both cat and human detected
- Stream live video to iOS devices for remote monitoring
- Automatically manage storage (keeps 10 newest recordings)
- Achieve **77.0% mAP50** on expanded dataset (2500+ images)

### Current Status: âœ… Phase 2 Complete
- âœ… 2-class object detection (human: 63.7%, cat: 90.4% mAP50)
- âœ… Trained model: YOLOv8s on 2500+ images
- âœ… **iOS App**: Remote viewer with video playback
- âœ… **Auto-recording**: Detects cat+human interactions
- âœ… **Storage management**: Auto-cleanup (max 10 videos)
- âœ… **Streaming server**: Mac/Jetson webcam â†’ iOS viewer
- â³ Jetson Nano deployment (next phase)

## ğŸ› ï¸ Hardware Requirements

- **NVIDIA Jetson Nano** (2GB or 4GB model)
- **USB Camera** or CSI Camera module
- **Power supply** (5V 4A recommended)
- **MicroSD card** (64GB+ recommended)
- Optional: External cooling fan

## ğŸ“‹ Software Requirements

### Current Development Environment
- **Python:** 3.14.0
- **PyTorch:** 2.9.0+cpu (CPU-only validated)
- **YOLOv8:** Ultralytics 8.3.222
- **OpenCV:** 4.12.0.88
- **Platform:** Windows 11

### For Deployment (Jetson Nano) - Future
- JetPack 4.6+ (includes Ubuntu 18.04, CUDA, cuDNN)
- Python 3.6+
- TensorRT optimization recommended

## ğŸš€ Quick Start

### âœ… Current Status: Streaming System Ready

**Trained Model Location:**  
`runs/best_accuracy/yolov8s_massive/weights/best.pt`

**Performance:**
- Overall mAP50: 77.0% (on 2500+ images)
- Cat Detection: 90.4% mAP50
- Human Detection: 63.7% mAP50
- Model: YOLOv8s

### ğŸ¥ Run the Streaming System

**1. Start Backend Server (Mac/Jetson):**
```bash
cd /path/to/hand-pet-interaction-detector
python3 ios_app/streaming_backend_server.py
```

**2. Stop Server:**
```bash
# Press Ctrl+C or run:
lsof -ti:5001 | xargs kill -9
```

**3. Run iOS App:**
- Open `ios_app/HandPetInteraction.xcodeproj` in Xcode
- Update server URL in Settings: `http://YOUR_MAC_IP:5001`
- Run on iPhone/iPad
- View live stream and recorded videos

### ğŸ“– Documentation Quick Links

| Document | Purpose |
|----------|---------|  
| **[ios_app/STREAMING_SETUP.md](ios_app/STREAMING_SETUP.md)** | â­ iOS App Setup & Streaming Guide |
| **[TRAINING_DOCUMENTATION.md](TRAINING_DOCUMENTATION.md)** | Complete training methodology & results |
| **[QUICK_TEST_GUIDE.md](QUICK_TEST_GUIDE.md)** | Test with webcam, images, or video |
| **[TERMINAL_RECORD.md](TERMINAL_RECORD.md)** | All commands used (reproducibility) |
| **[DATA_COLLECTION_GUIDE.md](DATA_COLLECTION_GUIDE.md)** | How data was collected & labeled |
| **[JETSON_SETUP.md](JETSON_SETUP.md)** | Deploy to Jetson Nano (future) |### ğŸ§ª Test the Model Now

**1. Test with Webcam:**
```powershell
& "C:\Users\ngtszch\AppData\Local\Programs\Python\Python314\python.exe" "test_model.py" --source webcam
```

**2. Test with Images:**
```powershell
& "C:\Users\ngtszch\AppData\Local\Programs\Python\Python314\python.exe" "test_model.py" --source "data\yolo_dataset\images\test"
```

**3. Test with Your Own Image:**
```powershell
& "C:\Users\ngtszch\AppData\Local\Programs\Python\Python314\python.exe" "test_model.py" --source "path\to\your\image.jpg"
```

See **[QUICK_TEST_GUIDE.md](QUICK_TEST_GUIDE.md)** for more testing options.

### ğŸ“ Want to Train Your Own Model?

See **[DATA_COLLECTION_GUIDE.md](DATA_COLLECTION_GUIDE.md)** for:
- How to collect cat images
- How to label them
- How to train the model

**Quick training command:**
```powershell
& "C:\Users\ngtszch\AppData\Local\Programs\Python\Python314\python.exe" "train_model.py" --data-dir "data\yolo_dataset" --epochs 50 --batch-size 4 --device cpu
```

## ğŸ“ Project Structure

```
hand-pet-interaction-detector/
â”œâ”€â”€ train_model.py          # Model training script
â”œâ”€â”€ test_model.py           # Model testing script
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ ios_app/               # â­ iOS App & Backend
â”‚   â”œâ”€â”€ streaming_backend_server.py  # Mac/Jetson streaming server
â”‚   â”œâ”€â”€ cleanup_videos.py            # Video storage management
â”‚   â”œâ”€â”€ STREAMING_SETUP.md           # Setup guide
â”‚   â”œâ”€â”€ StreamView.swift             # iOS stream viewer
â”‚   â”œâ”€â”€ ContentView.swift            # iOS main view
â”‚   â”œâ”€â”€ NetworkManager.swift         # iOS networking
â”‚   â””â”€â”€ VideosView.swift             # iOS video player
â”œâ”€â”€ recorded_videos/       # Auto-recorded interactions (max 10)
â”œâ”€â”€ runs/
â”‚   â””â”€â”€ best_accuracy/
â”‚       â””â”€â”€ yolov8s_massive/
â”‚           â””â”€â”€ weights/
â”‚               â””â”€â”€ best.pt        # Trained model (77% mAP50)
â””â”€â”€ data/
    â””â”€â”€ expanded_dataset/          # 2500+ annotated images
```

## âš™ï¸ Configuration

Edit `config/config.json` to customize:

```json
{
  "model_path": "models/best.pt",
  "confidence_threshold": 0.5,
  "touching_threshold": 50,
  "hitting_threshold": 15,
  "velocity_hitting_threshold": 100,
  "enable_sound_alert": true,
  "enable_logging": true,
  "camera_width": 640,
  "camera_height": 480,
  "camera_fps": 30
}
```

### Key Parameters:
- **touching_threshold**: Distance in pixels to detect touching (default: 50)
- **hitting_threshold**: Distance in pixels for hitting detection (default: 15)
- **velocity_hitting_threshold**: Human velocity threshold in px/s (default: 100)
- **confidence_threshold**: Minimum detection confidence (default: 0.5)

## ğŸ¥ Camera Setup

### USB Camera
```bash
# List available cameras
ls /dev/video*

# Test camera
python3 -c "import cv2; cap = cv2.VideoCapture(0); print(cap.isOpened())"
```

### CSI Camera (Raspberry Pi Camera)
```bash
# Use GStreamer pipeline in main.py
# Modify camera initialization to use CSI camera
```

## ğŸ”§ Optimization for Jetson Nano

### 1. Use FP16 Precision
The training script includes automatic FP16 optimization:
```bash
python train_model.py --optimize
```

### 2. Adjust Performance Mode
```bash
# Maximum performance
sudo nvpmodel -m 0
sudo jetson_clocks
```

### 3. Reduce Image Size
In `config/config.json`:
```json
{
  "camera_width": 416,
  "camera_height": 416
}
```

### 4. Use TensorRT (Advanced)
```bash
# Convert model to TensorRT
# See Jetson optimization guides
```

## ğŸ“Š Performance Metrics

Expected performance on Jetson Nano:
- **YOLOv5n**: ~15-20 FPS (416x416)
- **YOLOv8n**: ~12-18 FPS (416x416)
- **YOLOv5s**: ~8-12 FPS (640x640)

## ğŸ“± iOS App Features

### Live Streaming
- **Real-time view**: See Mac/Jetson webcam feed on iPhone/iPad
- **Detection overlay**: View human and cat detections in real-time
- **Recording indicator**: Shows when interaction is being recorded
- **Confidence scores**: Displays detection confidence levels

### Auto-Recording
- **Smart trigger**: Automatically records when cat AND human detected
- **Cooldown period**: 2-second timeout between recordings
- **Storage management**: Auto-cleanup keeps only 10 newest videos
- **Timestamp naming**: `interaction_YYYYMMDD_HHMMSS.mp4`

### Video Playback
- **Browse recordings**: List of all saved interaction videos
- **Thumbnail preview**: Visual preview of each recording
- **Built-in player**: Play videos directly in the app
- **Auto-refresh**: List updates automatically

### Configuration
- **Server URL**: Set Mac/Jetson IP address
- **Connection test**: Verify server connectivity
- **Port**: 5001 (configurable in server)

## ğŸ“ Training Tips

1. **Collect diverse data**: Different lighting, angles, pets, people
2. **Balance classes**: Equal samples of touching, hitting, no-interaction
3. **Augmentation**: YOLOv8 includes built-in augmentation
4. **Start small**: Begin with YOLOv8n, upgrade if needed
5. **Monitor training**: Use TensorBoard

```bash
tensorboard --logdir runs/train
```

## ğŸ› Troubleshooting

### Camera not detected
```bash
# Check camera connections
v4l2-ctl --list-devices

# Try different camera IDs
python3 main.py --camera 1
```

### Low FPS on Jetson
- Reduce image resolution
- Use smaller model (YOLOv8n)
- Enable maximum performance mode
- Close other applications

### Model not loading
- Check model path in `config/config.json`
- Ensure model is in `models/` directory
- Verify PyTorch version compatibility

### Out of memory on Jetson
- Reduce batch size during training
- Use swap space
- Close other applications

## ğŸ“š Additional Resources

- [YOLOv5 Documentation](https://github.com/ultralytics/yolov5)
- [YOLOv8 Documentation](https://docs.ultralytics.com/)
- [Jetson Nano Guide](https://developer.nvidia.com/embedded/learn/get-started-jetson-nano-devkit)
- [NVIDIA JetPack](https://developer.nvidia.com/embedded/jetpack)

## ğŸ¤ Contributing

This is a FYP project. Suggestions and improvements are welcome!

## ğŸ“„ License

This project is for educational purposes.

## ğŸ‘¤ Author

FYP Project - Human-Pet Interaction Detection System

## ğŸ™ Acknowledgments

- Ultralytics for YOLOv5/YOLOv8
- NVIDIA for Jetson platform
- OpenCV community

---

## ğŸ“ Support

For issues or questions about this project, please create an issue in the repository or contact the project maintainer.

**Good luck with your FYP! ğŸš€**
